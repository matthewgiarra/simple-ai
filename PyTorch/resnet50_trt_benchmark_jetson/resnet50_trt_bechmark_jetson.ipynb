{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matthew Giarra <matthew.giarra@jhuapl.edu>\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch2trt import torch2trt\n",
    "import time # for timing execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function times inference for a model\n",
    "def time_inference(model, data, niter):\n",
    "    \n",
    "    # Warm-up inference loop.\n",
    "    # Inference is often slowest on first iteration.\n",
    "    # \"Warm up\" takes care of that.\n",
    "    with torch.no_grad(): \n",
    "        for t in range(3):\n",
    "            output = model(data)\n",
    "    \n",
    "    # Timed inference loop\n",
    "    tic = time.perf_counter() # Start a timer\n",
    "    with torch.no_grad():  # torch.no_grad() turns off gradient calculations for faster performance\n",
    "        for t in range(niter): # Loop niter times\n",
    "            output = model(data) # RUN THE INFERENCE\n",
    "    toc = time.perf_counter() # Stop the timer\n",
    "    exe_sec = toc-tic # Seconds elapsed\n",
    "    FPS = niter / (exe_sec) # Frames per second\n",
    "    return exe_sec, FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input data\n",
    "input_tensor_cpu = torch.rand([1,3,224,224])\n",
    "input_tensor_gpu_full  = input_tensor_cpu.to('cuda')\n",
    "input_tensor_gpu_half  = input_tensor_gpu_full.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # Get models  # # # # #  \n",
    "\n",
    "# Pytorch, Float32\n",
    "model_pytorch_fp32 = torchvision.models.resnet50(pretrained=False).eval().to('cuda')\n",
    "\n",
    "# Pytorch, Float16\n",
    "model_pytorch_fp16 = torchvision.models.resnet50(pretrained=False).eval().to('cuda').half()\n",
    "\n",
    "# # # # # Convert models to TensorRT # # # # #\n",
    "\n",
    "# TensorRT, Float32\n",
    "model_trt_fp32 = torch2trt(model_pytorch_fp32, [input_tensor_gpu_full], fp16_mode=False)\n",
    "\n",
    "# TensorRT, Float16\n",
    "model_trt_fp16 = torch2trt(model_pytorch_fp16, [input_tensor_gpu_half], fp16_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results summary (500 images)\n",
      "===============\n",
      "PyTorch  (FP32): 13.06 seconds  (38 FPS),  1.00x PyTorch FP32 speed\n",
      "PyTorch  (FP16): 7.16 seconds  (69 FPS),  1.82x PyTorch FP32 speed\n",
      "TensorRT (FP32): 4.24 seconds  (117 FPS),  3.08x PyTorch FP32 speed\n",
      "TensorRT (FP16): 1.46 seconds  (342 FPS),  8.95x PyTorch FP32 speed\n"
     ]
    }
   ],
   "source": [
    "# Vectors to hold results\n",
    "times_list = []\n",
    "fps_list = []\n",
    "names_list = []\n",
    "\n",
    "# number of iterations per inference trial\n",
    "niter = 500\n",
    "\n",
    "# Do all the inferences\n",
    "sec, fps = time_inference(model_pytorch_fp32, input_tensor_gpu_full, niter); times_list.append(sec); fps_list.append(fps); names_list.append(\"PyTorch  (FP32)\")\n",
    "sec, fps = time_inference(model_pytorch_fp16, input_tensor_gpu_half, niter); times_list.append(sec); fps_list.append(fps); names_list.append(\"PyTorch  (FP16)\") \n",
    "sec, fps = time_inference(model_trt_fp32, input_tensor_gpu_full, niter); times_list.append(sec); fps_list.append(fps); names_list.append(\"TensorRT (FP32)\")\n",
    "sec, fps = time_inference(model_trt_fp16, input_tensor_gpu_half, niter); times_list.append(sec); fps_list.append(fps); names_list.append(\"TensorRT (FP16)\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nResults summary (%d images)\\n===============\" % (niter) )\n",
    "for name, exe_sec, fps in zip(names_list, times_list, fps_list):\n",
    "    print(\"%s: %0.2f seconds  (%d FPS),  %0.2fx PyTorch FP32 speed\" % (name, exe_sec, fps, fps/fps_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current results using Jetson AGX Xavier\n",
    "\n",
    "### Configuration\n",
    "- Platform: NVIDIA Jetson AGX Xavier\n",
    "- Jetpack SDK: 4.4 ([L4T R32.4.3](https://developer.nvidia.com/embedded/jetpack))\n",
    "- Power mode: [MAXN](https://www.jetsonhacks.com/2018/10/07/nvpmodel-nvidia-jetson-agx-xavier-developer-kit/) (`$ sudo nvpmodel -m 0`) \n",
    "- Docker source image: `nvcr.io/nvidia/l4t-ml:r32.4.3-py3`  ([link](https://ngc.nvidia.com/catalog/containers/nvidia:l4t-ml))\n",
    "- NVIDIA `torch2trt` for converting PyTorch models to TensorRT ([link](https://github.com/NVIDIA-AI-IOT/torch2trt))\n",
    "\n",
    "### Results\n",
    "| Model| Execution time (sec) | Throughput (FPS) | Speed-up (vs. PyTorch FP32) |\n",
    "|:----------:|:----------------------:|:----------:|:--------:|\n",
    "|   PyTorch  (FP32) |        13.06        |   38   |     1.00 |\n",
    "|   PyTorch  (FP16) |        7.16        |   69   |     1.82 |\n",
    "|   TensorRT (FP32) |        4.24        |   117   |     3.08 |\n",
    "|   TensorRT (FP16) |        1.46        |   342   |     8.95 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results indicate that inference using `Float16` precision on Resnet50 yields about a 2x speedup compared to inference using `Float32` precision in PyTorch. Moreover, converting the model to TensorRT results in speed-ups of 3x and 9x for `Float32` and `Float16`, respectively, compared to `Float32` in PyTorch. This is significant, but still falls far short of the results posted on the [NVIDIA Developer Blog](https://developer.nvidia.com/blog/jetson-xavier-nx-the-worlds-smallest-ai-supercomputer/) (shown below), which are over 5x faster than our best performance here (1941 FPS vs. our 342 FPS for 224x224 images).  \n",
    "\n",
    "NVIDIA's results use `int8` precision for inference versus `FP16` used by our fastest model. This discrepancy probably accounts for a large fraction of the difference in performance. I need to read more to figure out how to convert models to `int8`.\n",
    "\n",
    "![Image](https://developer.download.nvidia.com/devblogs/inferencing-performance.png)\n",
    "\n",
    "## Caveats\n",
    "The TensorRT models appear not to run with batch sizes greater than 1. The code doesn't crash, but the inference lines execute immediately for any batch size > 1. I verified via `tegrastats` that the device is not running out of memory. I currently don't know the cause of this issue, and it would be interesting to compare inference performance using larger batches.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
