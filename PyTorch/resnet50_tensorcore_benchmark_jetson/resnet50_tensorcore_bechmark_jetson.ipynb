{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "AMP (opt level O3): 9.62 seconds\n",
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "AMP (opt level O2): 9.67 seconds\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "AMP (opt level O1): 9.66 seconds\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "AMP (opt level O0): 9.73 seconds\n",
      "Float32: 9.70 seconds\n",
      "\n",
      "Results summary (3000 images)\n",
      "===============\n",
      "AMP O3: 9.62 seconds  (1.01x full precision speed)\n",
      "AMP O2: 9.67 seconds  (1.00x full precision speed)\n",
      "AMP O1: 9.66 seconds  (1.00x full precision speed)\n",
      "AMP O0: 9.73 seconds  (1.00x full precision speed)\n",
      "Float32: 9.70 seconds  (1.00x full precision speed)\n"
     ]
    }
   ],
   "source": [
    "# Matthew Giarra <matthew.giarra@jhuapl.edu>\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import apex.amp as amp\n",
    "import time # for timing execution\n",
    "\n",
    "# Number of images per batch\n",
    "batch_size = 100\n",
    "\n",
    "# Number of iterations\n",
    "niter = 30\n",
    "\n",
    "# Results vectors\n",
    "results_list = []\n",
    "results_names = []\n",
    "\n",
    "# Make data\n",
    "input_batch_cpu = torch.rand([batch_size, 3, 224, 224], dtype=torch.float32)\n",
    "input_batch_gpu_full  = input_batch_cpu.to('cuda')\n",
    "input_batch_gpu_half  = input_batch_gpu_full.half()\n",
    "\n",
    "# # # # # Inference with automatic mixed precision (AMP) via APEX  # # # # #  \n",
    "\n",
    "# Run each of the APEX AMP optimization levels\n",
    "for opt_level in [\"O3\", \"O2\", \"O1\", \"O0\"]:\n",
    "    model = torchvision.models.resnet50(pretrained=False).eval().to('cuda')\n",
    "    model_amp = amp.initialize(model, opt_level=opt_level)\n",
    "    \n",
    "    # Warm up\n",
    "    with torch.no_grad():\n",
    "        for t in range(3):\n",
    "            output_gpu = model_amp(input_batch_gpu_half)\n",
    "\n",
    "    # Run inference on the batch of images\n",
    "    # torch.no_grad() turns off gradient calculations for faster performance\n",
    "    tic = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for t in range(niter):\n",
    "            output_gpu = model_amp(input_batch_gpu_half)\n",
    "    # Execution time\n",
    "    toc = time.perf_counter()\n",
    "    print(\"AMP (opt level %s): %0.2f seconds\" % (opt_level, toc-tic))\n",
    "    \n",
    "    # Results\n",
    "    results_list.append(toc-tic)\n",
    "    results_names.append('AMP ' + opt_level)\n",
    "        \n",
    "    \n",
    "    \n",
    "# # # # # Inference with full precision (Float32) # # # # #        \n",
    "\n",
    "# Load the model\n",
    "model = torchvision.models.resnet50(pretrained=False).eval().to('cuda')\n",
    "\n",
    "# Warm up\n",
    "with torch.no_grad():\n",
    "    for t in range(3):\n",
    "        output_gpu = model(input_batch_gpu_full)\n",
    "      \n",
    "# Run inference on the batch of images\n",
    "# torch.no_grad() turns off gradient calculations for faster performance\n",
    "tic = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    for t in range(niter):\n",
    "        output_gpu = model(input_batch_gpu_full)\n",
    "toc = time.perf_counter()\n",
    "print(\"Float32: %0.2f seconds\" % (toc-tic))\n",
    "\n",
    "# Results\n",
    "results_list.append(toc-tic)\n",
    "results_names.append('Float32')\n",
    "\n",
    "print(\"\\nResults summary (%d images)\\n===============\" % (batch_size * niter) )\n",
    "for name, result in zip(results_names, results_list):\n",
    "    print(\"%s: %0.2f seconds  (%0.2fx full precision speed)\" % (name, result, results_list[-1]/result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current results using Jetson AGX Xavier\n",
    "\n",
    "### Configuration\n",
    "- Platform: NVIDIA Jetson AGX Xavier\n",
    "- Jetpack SDK: 4.4 ([L4T R32.4.3](https://developer.nvidia.com/embedded/jetpack))\n",
    "- Power mode: [MAXN](https://www.jetsonhacks.com/2018/10/07/nvpmodel-nvidia-jetson-agx-xavier-developer-kit/) (`$ sudo nvpmodel -m 0`) \n",
    "- Docker source image: `nvcr.io/nvidia/l4t-ml:r32.4.3-py3`  ([link](https://ngc.nvidia.com/catalog/containers/nvidia:l4t-ml))\n",
    "- Apex build: [full build](https://github.com/NVIDIA/apex#quick-start)\n",
    "\n",
    "### Results\n",
    "| Precision| Execution time (sec) | Throughput (FPS) | Speed-up |\n",
    "|:----------:|:----------------------:|:----------:|:--------:|\n",
    "|   AMP O3 |        9.72        |   308   |     1.00 |\n",
    "|   AMP O2 |        9.67        |   310   |     1.01 |\n",
    "|   AMP O1 |        9.65        |   310   |     1.01 |\n",
    "|   AMP O0 |        9.72        |   308   |     1.00 |\n",
    "|   Float32 |        9.70        |   309   |     1.00 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with published results\n",
    "These results are much worse than what's posted on the [NVIDIA Developer Blog](https://developer.nvidia.com/blog/jetson-xavier-nx-the-worlds-smallest-ai-supercomputer/). Specifically, the throughput is around 16% of their reported throughput using Resnet50 on images of the same size (1941 FPS for 224x224 images). \n",
    "\n",
    "\n",
    "![Image](https://developer.download.nvidia.com/devblogs/inferencing-performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why I think it should be faster\n",
    "\n",
    "Our results indicate that inference using mixed precision (or even pure `Float16`) on `Resnet50` yields no speedup compared to inference using `Float32` precision. I've experimented with various batch sizes, and the results are not exceptionally different from what's above. This is contrary to my expectation. I think we should see a speed-up because the tensor cores should be invoked under the following circumstances, which I believe I've met: \n",
    "\n",
    "1. Device has tensor cores ([NVIDIA Jetson AGX has Volta architecture](http://info.nvidia.com/rs/156-OFN-742/images/Jetson_AGX_Xavier_New_Era_Autonomous_Machines.pdf))\n",
    "2. Much of the work in the feed-forward process consists of convolutional layers, which [should invoke tensor cores for FP16 operations](https://nvidia.github.io/apex/amp.html#o1-mixed-precision-recommended-for-typical-use).\n",
    "\n",
    "3. I'm using cuDNN 8.0, and [for \"cudnn 7.3 and later, convolutions should use TensorCores for FP16 inputs\"](https://discuss.pytorch.org/t/cnn-fp16-slower-than-fp32-on-tesla-p100/12146/4):\n",
    "\n",
    "    ```python\n",
    "    >>> import torch\n",
    "    >>> print(torch.backends.cudnn.version())\n",
    "    8000\n",
    "    ```      \n",
    "\n",
    "\n",
    "4. The number of input and output channels in each `Conv2d` layer is a multiple of 8 (except the 3-channel input to the first layer), which is a [requirement for tensor cores](https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf). So are the dimensions of the fully connected `linear` layers. You can verify this by inspecting the output of the following commands:\n",
    "\n",
    "    ```python\n",
    "    >>> import torchvision\n",
    "    >>> print(torchvision.models.resnet50())\n",
    "\n",
    "    ResNet(\n",
    "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "      (layer1): Sequential(\n",
    "        (0): Bottleneck(\n",
    "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          )\n",
    "        )\n",
    "        (1): Bottleneck(\n",
    "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "          .\n",
    "          .\n",
    "          .\n",
    "\n",
    "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
    "    )\n",
    "    ```\n",
    "etc. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
